{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshadaRaut234/Predicton-of-DR-using-XAI-and-CNN/blob/main/DRprediction_intern.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI1pU-MyRjjV",
        "outputId": "23957f2a-9e03-42af-e21f-fa7c0424aa46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LK5PdMwpR5na"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from PIL import Image, ImageEnhance, ImageOps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7R_UefxhSBpb"
      },
      "outputs": [],
      "source": [
        "# Path to the ZIP file in Google Drive\n",
        "zip_file_path = '/content/drive/MyDrive/archive.zip'\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/extracted_images')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count images in each folder (class)\n",
        "class_counts = {}\n",
        "for folder_name in os.listdir('/content/extracted_images/dr_unified_v2/dr_unified_v2/train'):\n",
        "    folder_path = os.path.join('/content/extracted_images/dr_unified_v2/dr_unified_v2/train', folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        num_images = len(os.listdir(folder_path))\n",
        "        class_counts[folder_name] = num_images\n",
        "\n",
        "# Print the number of images in each class\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f'Class {class_name}: {count} images')\n"
      ],
      "metadata": {
        "id": "zg1ttcZ4kWnX",
        "outputId": "260db53d-b759-4e0e-d4c0-55b839c39171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 2: 12119 images\n",
            "Class 3: 1006 images\n",
            "Class 1: 3706 images\n",
            "Class 4: 2002 images\n",
            "Class 0: 55162 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target number of images for each class (e.g., 2000 images per class)\n",
        "target_num_images = 2000  # Change to 2000 for all classes except Class 3\n",
        "\n",
        "# Create a new directory to store the downsampled dataset if it doesn't exist\n",
        "downsampled_dir = '/content/downsampled_images'\n",
        "if not os.path.exists(downsampled_dir):\n",
        "    os.makedirs(downsampled_dir)\n",
        "else:\n",
        "    # Remove existing images to start fresh\n",
        "    shutil.rmtree(downsampled_dir)\n",
        "    os.makedirs(downsampled_dir)\n",
        "# Function for image augmentation\n",
        "def augment_image(image):\n",
        "    # Randomly apply various augmentations\n",
        "    if random.random() > 0.5:\n",
        "        image = ImageOps.mirror(image)\n",
        "    if random.random() > 0.5:\n",
        "        image = ImageOps.flip(image)\n",
        "    if random.random() > 0.5:\n",
        "        enhancer = ImageEnhance.Brightness(image)\n",
        "        image = enhancer.enhance(random.uniform(0.5, 1.5))\n",
        "    if random.random() > 0.5:\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(random.uniform(0.5, 1.5))\n",
        "    if random.random() > 0.5:\n",
        "        enhancer = ImageEnhance.Color(image)\n",
        "        image = enhancer.enhance(random.uniform(0.5, 1.5))\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "tp2VFlNwkbp6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the class_counts dictionary is not empty\n",
        "if class_counts:\n",
        "    # Iterate over each class and copy exactly 2000 images to the downsampled dataset\n",
        "    for class_name, num_images in class_counts.items():\n",
        "        print(f'Processing class {class_name} with {num_images} images')  # Debugging statement\n",
        "        class_dir = os.path.join('/content/extracted_images/dr_unified_v2/dr_unified_v2/train', class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            # Create the directory for the downsampled dataset\n",
        "            os.makedirs(os.path.join(downsampled_dir, class_name), exist_ok=True)\n",
        "\n",
        "            if class_name == '3':\n",
        "                # For Class 3, copy all images and then perform augmentation to reach 2000\n",
        "                sampled_images = os.listdir(class_dir)\n",
        "            else:\n",
        "                # For other classes, randomly sample exactly 2000 images\n",
        "                sampled_images = random.sample(os.listdir(class_dir), target_num_images)\n",
        "\n",
        "            # Copy sampled images to the downsampled dataset directory\n",
        "            for image_name in sampled_images:\n",
        "                src_path = os.path.join(class_dir, image_name)\n",
        "                dst_path = os.path.join(downsampled_dir, class_name, image_name)\n",
        "                if os.path.isfile(src_path):  # Check if the item is a file\n",
        "                    shutil.copy(src_path, dst_path)\n",
        "\n",
        "            # Ensure the downsampling for other classes\n",
        "            if class_name != '3':\n",
        "                existing_images = os.listdir(os.path.join(downsampled_dir, class_name))\n",
        "                current_count = len(existing_images)\n",
        "                if current_count > target_num_images:\n",
        "                    extra_images = random.sample(existing_images, current_count - target_num_images)\n",
        "                    for image_name in extra_images:\n",
        "                        os.remove(os.path.join(downsampled_dir, class_name, image_name))\n",
        "\n",
        "            # Augment images for Class 3 if needed\n",
        "            if class_name == '3':\n",
        "                existing_images = os.listdir(os.path.join(downsampled_dir, class_name))\n",
        "                current_count = len(existing_images)\n",
        "                while current_count < target_num_images:\n",
        "                    for image_name in existing_images:\n",
        "                        if current_count >= target_num_images:\n",
        "                            break\n",
        "                        src_path = os.path.join(downsampled_dir, class_name, image_name)\n",
        "                        image = Image.open(src_path)\n",
        "                        augmented_image = augment_image(image)\n",
        "                        augmented_image_name = f'aug_{current_count}.png'\n",
        "                        augmented_image_path = os.path.join(downsampled_dir, class_name, augmented_image_name)\n",
        "                        augmented_image.save(augmented_image_path)\n",
        "                        current_count += 1\n",
        "\n",
        "    print('Dataset downsampling and upsampling completed successfully.')\n",
        "\n",
        "    # Count images in each class after downsampling and upsampling\n",
        "    downsampled_class_counts = {}\n",
        "    for class_name in os.listdir(downsampled_dir):\n",
        "        class_path = os.path.join(downsampled_dir, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            num_images = len(os.listdir(class_path))\n",
        "            downsampled_class_counts[class_name] = num_images\n",
        "\n",
        "    # Print the number of images in each class after downsampling and upsampling\n",
        "    print('\\nNumber of images in each class after downsampling and upsampling:')\n",
        "    for class_name, num_images in downsampled_class_counts.items():\n",
        "        print(f'Class {class_name}: {num_images} images')\n",
        "else:\n",
        "    print('Error: class_counts dictionary is empty. Make sure it is populated before running this code.')\n"
      ],
      "metadata": {
        "id": "CMII2cBEke3E",
        "outputId": "7c851716-0aea-4f02-eb14-72afdc377a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing class 2 with 12119 images\n",
            "Processing class 3 with 1006 images\n",
            "Processing class 1 with 3706 images\n",
            "Processing class 4 with 2002 images\n",
            "Processing class 0 with 55162 images\n",
            "Dataset downsampling and upsampling completed successfully.\n",
            "\n",
            "Number of images in each class after downsampling and upsampling:\n",
            "Class 2: 2000 images\n",
            "Class 3: 2000 images\n",
            "Class 1: 2000 images\n",
            "Class 4: 2000 images\n",
            "Class 0: 2000 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "# Define the path to save the split data\n",
        "split_data_dir = '/content/split_data'\n",
        "os.makedirs(split_data_dir, exist_ok=True)\n",
        "\n",
        "# Define the ratio for the train-test-validation split\n",
        "train_ratio = 0.7\n",
        "test_ratio = 0.2\n",
        "val_ratio = 0.1\n",
        "\n",
        "# Function to split data for each class\n",
        "def split_data_for_class(class_name):\n",
        "    class_dir = os.path.join(downsampled_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        # List all images\n",
        "        images = os.listdir(class_dir)\n",
        "        total_images = len(images)\n",
        "\n",
        "        # Calculate the number of images for each split\n",
        "        train_count = int(total_images * train_ratio)\n",
        "        test_count = int(total_images * test_ratio)\n",
        "        val_count = total_images - train_count - test_count\n",
        "\n",
        "        # Create directories for train, test, and validation sets\n",
        "        train_dir = os.path.join(split_data_dir, 'train', class_name)\n",
        "        test_dir = os.path.join(split_data_dir, 'test', class_name)\n",
        "        val_dir = os.path.join(split_data_dir, 'val', class_name)\n",
        "        os.makedirs(train_dir, exist_ok=True)\n",
        "        os.makedirs(test_dir, exist_ok=True)\n",
        "        os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "        # Split the data\n",
        "        train_images, test_val_images = train_test_split(images, test_size=(test_count + val_count), random_state=42)\n",
        "        test_images, val_images = train_test_split(test_val_images, test_size=(val_count / (test_count + val_count)), random_state=42)\n",
        "\n",
        "        # Copy images to the respective directories\n",
        "        for image_name in train_images:\n",
        "            src_path = os.path.join(class_dir, image_name)\n",
        "            dst_path = os.path.join(train_dir, image_name)\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "        for image_name in test_images:\n",
        "            src_path = os.path.join(class_dir, image_name)\n",
        "            dst_path = os.path.join(test_dir, image_name)\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "        for image_name in val_images:\n",
        "            src_path = os.path.join(class_dir, image_name)\n",
        "            dst_path = os.path.join(val_dir, image_name)\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "        print(f'Data split for Class {class_name}')\n",
        "\n",
        "        # Print the number of images in training, testing, and validation sets for each class\n",
        "        print(f'Class {class_name}:')\n",
        "        print(f'  Training set: {len(train_images)} images')\n",
        "        print(f'  Testing set: {len(test_images)} images')\n",
        "        print(f'  Validation set: {len(val_images)} images')\n",
        "\n",
        "# Split data for each class\n",
        "for class_name in downsampled_class_counts.keys():\n",
        "    split_data_for_class(class_name)\n",
        "\n",
        "# Print confirmation\n",
        "print('Data splitting completed successfully.')\n"
      ],
      "metadata": {
        "id": "3jeQacklkfwo",
        "outputId": "65283ef2-8c61-4e93-e980-b9dce672d5e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split for Class 2\n",
            "Class 2:\n",
            "  Training set: 1400 images\n",
            "  Testing set: 400 images\n",
            "  Validation set: 200 images\n",
            "Data split for Class 3\n",
            "Class 3:\n",
            "  Training set: 1400 images\n",
            "  Testing set: 400 images\n",
            "  Validation set: 200 images\n",
            "Data split for Class 1\n",
            "Class 1:\n",
            "  Training set: 1400 images\n",
            "  Testing set: 400 images\n",
            "  Validation set: 200 images\n",
            "Data split for Class 4\n",
            "Class 4:\n",
            "  Training set: 1400 images\n",
            "  Testing set: 400 images\n",
            "  Validation set: 200 images\n",
            "Data split for Class 0\n",
            "Class 0:\n",
            "  Training set: 1400 images\n",
            "  Testing set: 400 images\n",
            "  Validation set: 200 images\n",
            "Data splitting completed successfully.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}